{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概念定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法基本思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一步：**确定模型的假设函数和损失函数\n",
    "\n",
    "假设函数：$$h_\\theta(x)=\\theta_0+\\theta_1x$$\n",
    "损失函数：$$J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "损失函数其他写法：$$J(\\theta)=\\frac{1}{2m}(Y-X\\theta)^T(Y-X\\theta)$$\n",
    "其中 $\\theta=(\\theta_1,\\theta_0)$\n",
    "\n",
    "**第二步：**相关参数的初始化，包括：参数、算法终止距离和步长\n",
    "\n",
    "参数 $\\theta$ —— 最终所求使得J最小的 $\\theta$ 值\n",
    "\n",
    "步长 $\\alpha$ —— 学习率\n",
    "\n",
    "**第三步：**确定当前位置损失函数的梯度\n",
    "\n",
    "$$gradients=\\frac{\\partial J(\\theta)}{\\partial\\theta}=\\frac{1}{m}X^T(X\\theta-Y)$$\n",
    "\n",
    "**第四步：**用步长乘以梯度，得到当前位置下降的距离\n",
    "\n",
    "$$\\theta_{new}=\\theta-\\alpha* gradients$$\n",
    "\n",
    "**第五步：**确定是否所有参数梯度下降的距离都小于算法终止距离，如果小于则算法终止，或是否到达迭代次数上限，如果到达则算法终止，否则进行下一步\n",
    "\n",
    "**第六步：**更新所有参数，更新完毕转到步骤1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BGD是对样本整体做梯度下降\n",
    "\n",
    "SGD 和 MBGD都是 BGD的特殊情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机梯度下降（SGD）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基本思想**\n",
    "\n",
    "对训练集中每个样本做梯度下降\n",
    "\n",
    "**优缺点**\n",
    "\n",
    "优点：计算速度快\n",
    "缺点：收敛性能不好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基本思想**\n",
    "\n",
    "将一个大的训练集随机均切分成若干子集，对每个子集做梯度下降\n",
    "\n",
    "mini-batch大小的确定：\n",
    "\n",
    "- 不能太大，太大会使得训练更快，但泛化能力下降：bacth-size更大，估计的梯度越可靠，需要迭代的步数也越小。可以提高学习率，从而放大梯度噪声的贡献。\n",
    "- 不能太小，太小的batch梯度估计值的方差非常大，因此需要非常小的学习率来维持稳定性，如果学习速率过大，则导致步长的变化剧烈。由于需要降低学习速率，因此需要消耗更多的迭代次数来训练，虽然每一轮迭代中计算梯度估计值的时间大幅度降低了，但是总的运行时间还是非常大。\n",
    "\n",
    "**优缺点**\n",
    "\n",
    "优点：训练集较大时计算速度比BGD快很多\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有一些算法可以加快梯度下降的速度，减轻梯度下降过程中的摆动现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动量梯度下降（GD with momentum）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基本思想**\n",
    "\n",
    "使用指数加权平均的方式更新梯度，以 mini-batch为例：\n",
    "\n",
    "$$V_{d\\omega},V_{db} = 0$$\n",
    "\n",
    "$$On\\ iteration\\ t:\\ Compute\\ d\\omega,\\ db\\ on\\ mini-batch$$\n",
    "\n",
    "$$V_{d\\omega} = \\beta V_{d\\omega}+(1-\\beta)d\\omega$$\n",
    "\n",
    "$$V_{db} = \\beta V_{db}+(1-\\beta)db$$\n",
    "\n",
    "$$\\omega = \\omega - \\alpha V_{d\\omega}$$\n",
    "\n",
    "$$b = b - \\alpha V_{db}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量的写法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{\\theta} = 0$$\n",
    "\n",
    "$$On\\ iteration\\ t:\\ Compute\\ d\\theta\\ on\\ mini-batch$$\n",
    "\n",
    "$$V_{d\\theta} = \\beta V_{d\\theta}+(1-\\beta)d\\theta$$\n",
    "\n",
    "$$\\theta = \\theta - \\alpha V_{d\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**超参**\n",
    "- 学习率 $\\alpha$\n",
    "- $\\beta$\n",
    "\n",
    "一般 $\\beta=0.9$ 时就能取得较好效果，不过可以尝试其他值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基本思想**\n",
    "\n",
    "与Momentum相同，都是用指数加权平均的方式更新梯度，不同之处在于更新的方式，以 mini-batch为例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_{d\\omega},S_{db} = 0$$\n",
    "\n",
    "$$On\\ iteration\\ t:\\ Compute\\ d\\omega,\\ db\\ on\\ mini-batch$$\n",
    "\n",
    "$$S_{d\\omega} = \\beta S_{d\\omega}+(1-\\beta)d\\omega^2$$\n",
    "\n",
    "$$S_{db} = \\beta S_{db}+(1-\\beta)db^2$$\n",
    "\n",
    "$$\\omega = \\omega - \\alpha\\frac{d\\omega}{\\sqrt{S_{d\\omega}+\\varepsilon}}$$\n",
    "\n",
    "$$b = b - \\alpha \\frac{db}{\\sqrt{S_{db}+\\varepsilon}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $\\varepsilon$ 作用为保证分母不为0，通常取值 $10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量的写法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_{\\theta} = 0$$\n",
    "\n",
    "$$On\\ iteration\\ t:\\ Compute\\ d\\theta\\ on\\ mini-batch$$\n",
    "\n",
    "$$S_{d\\theta} = \\beta S_{d\\theta}+(1-\\beta)d\\theta^2$$\n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\frac{d\\theta}{\\sqrt{S_{d\\theta}+\\varepsilon}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基本思想**\n",
    "\n",
    "Adam是momentum和RMSprop的结合，以 mini-batch为例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{d\\omega}, V_{db}, S_{d\\omega}, S_{db} = 0$$\n",
    "\n",
    "$$On\\ iteration\\ t:\\ Compute\\ d\\omega,\\ db\\ on\\ mini-batch$$\n",
    "\n",
    "$$V_{d\\omega} = \\beta_1 V_{d\\omega}+(1-\\beta_1)d\\omega$$\n",
    "\n",
    "$$V_{db} = \\beta_1 V_{db}+(1-\\beta_1)db$$\n",
    "\n",
    "$$S_{d\\omega} = \\beta_2 S_{d\\omega}+(1-\\beta_2)d\\omega^2$$\n",
    "\n",
    "$$S_{db} = \\beta_2 S_{db}+(1-\\beta_2)db^2$$\n",
    "\n",
    "$$V_{d\\omega}^{corrected} = \\frac{V_{d\\omega}}{1-\\beta_1^t}, V_{db}^{corrected} = \\frac{V_{db}}{1-\\beta_1^t}$$\n",
    "\n",
    "$$S_{d\\omega}^{corrected} = \\frac{S_{d\\omega}}{1-\\beta_2^t}, S_{db}^{corrected} = \\frac{S_{db}}{1-\\beta_2^t}$$\n",
    "\n",
    "$$\\omega = \\omega - \\alpha\\frac{V_{d\\omega}^{corrected}}{\\sqrt{S_{d\\omega}^{corrected}+\\varepsilon}}$$\n",
    "\n",
    "$$b = b - \\alpha \\frac{V_{db}^{corrected}}{\\sqrt{S_{db}^{corrected}+\\varepsilon}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量的写法："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_{\\theta},S_{\\theta} = 0$$\n",
    "\n",
    "$$On\\ iteration\\ t:\\ Compute\\ d\\theta\\ on\\ mini-batch$$\n",
    "\n",
    "$$V_{d\\theta} = \\beta_1 V_{d\\theta}+(1-\\beta_1)d\\theta$$\n",
    "\n",
    "$$S_{d\\theta} = \\beta_2 S_{d\\theta}+(1-\\beta_2)d\\theta^2$$\n",
    "\n",
    "$$V_{d\\theta}^{corrected} = \\frac{V_{d\\theta}}{1-\\beta_1^t}$$\n",
    "\n",
    "$$S_{d\\theta}^{corrected} = \\frac{S_{d\\theta}}{1-\\beta_2^t}$$\n",
    "\n",
    "$$\\theta = \\theta - \\alpha\\frac{V_{d\\theta}^{corrected}}{\\sqrt{S_{d\\theta}^{corrected}+\\varepsilon}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习率衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基本思想**\n",
    "\n",
    "在学习过程中慢慢减少学习率\n",
    "- 前期，学习率较大，步长大，有利于快速收敛（前期学习率小的话需要花费更多迭代去收敛）\n",
    "- 后期，学习率较小，步长小，有利于更好收敛（后期学习率大的话则收敛的效果不好）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**衰减公式**\n",
    "\n",
    "1. $\\alpha = \\frac{1}{1+decayRate*epoch}\\alpha_0$ ，其中epoch为迭代次数，decayRate 和 $\\alpha_0$ 是超参。\n",
    "\n",
    "2. 指数衰减——$\\alpha = 0.95^{epoch}\\alpha_0$ \n",
    "\n",
    "\n",
    "3. 其他——$\\alpha = \\frac{k}{\\sqrt{epoch}}\\alpha_0$ or $\\alpha = \\frac{k}{\\sqrt{t}}\\alpha_0$，其中t为mini-batch参数\n",
    "\n",
    "\n",
    "4. 离散下降——每次迭代都为上一个 $\\alpha$ 的一半"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 面临的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "局部最优和鞍点"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

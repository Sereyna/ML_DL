{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso（L1）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法基本思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lasso regression = linear regression + L1 regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**假设函数**（同LR）\n",
    "$$h_\\theta(x)=\\theta_0+\\theta_1x = \\theta x$$\n",
    "\n",
    "**损失函数**\n",
    "$$J_0(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "$$J(\\theta)=J_0(\\theta)+\\lambda\\sum_{j=1}^n|\\theta_j|$$\n",
    "\n",
    "$$=J_0(\\theta)+\\lambda\\lVert\\theta_j\\rVert_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拟合算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为损失函数不是处处可导，所以无法求导解出，有两种解法：\n",
    "- 坐标下降法\n",
    "- 最小角回归\n",
    "\n",
    "在sklearn中`Lasso`类的实现使用了 `coordinate descent` （坐标下降算法）来拟合系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 坐标下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "坐标下降法的核心与它的名称一样，就是沿着某一个坐标轴方向，通过一次一次的迭代更新权重系数的值，来渐渐逼近最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**具体步骤**\n",
    "\n",
    "**第一步：**初始化权重系数 $\\theta$\n",
    "\n",
    "**第二步：**遍历所有权重系数，依次将其中一个权重系数当作变量，其他权重系数固定为上一次计算的结果当作常量，求出当前条件下只有一个权重系数变量的情况下的最优解。\n",
    "\n",
    "在第 k 次迭代时，更新权重系数的方法如下：\n",
    "$$\\theta_1^k = \\underset{\\theta_1}{argmin}(Cost(\\theta_1, \\theta_2^{k-1},\\cdots, \\theta_n^{k-1}))$$\n",
    "\n",
    "$$\\theta_2^k = \\underset{\\theta_2}{argmin}(Cost(\\theta_1^k, \\theta_2,\\cdots, \\theta_n^{k-1}))$$\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "$$\\theta_n^k = \\underset{\\theta_n}{argmin}(Cost(\\theta_1^k, \\theta_2^k,\\cdots, \\theta_n))$$\n",
    "\n",
    "**第三步：第二步**为一次完整迭代，当所有权重系数的变化不大或者到达最大迭代次数时，结束迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最小角回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最小角回归法是一种特征选择的方法，计算出每个特征的相关性，通过数学公式逐渐计算出最优解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**具体步骤**\n",
    "\n",
    "**第一步：**初始化权重系数 $\\theta$，例如初始化为零向量。\n",
    "\n",
    "**第二步：**初始化残差向量 $residual$ 为目标标签向量 $y - X\\theta$，由于此时 $\\theta$ 为零向量，所以残差向量等于目标标签向量 $y$\n",
    "\n",
    "**第三步：**选择一个与残差向量相关性最大的特征向量 $x_i$ ，沿着特征向量 $x_i$ 的方向找到一组权重系数 $\\theta$，出现另一个与残差向量相关性最大的特征向量 $x_j$ 使得新的残差向量 $residual$ 与这两个特征向量的相关性相等（即残差向量等于这两个特征向量的角平分向量上），重新计算残差向量。\n",
    "\n",
    "**第四步：**重复**第三步**继续找到一组权重系数 $\\theta$，使得第三个与残差向量相关性最大的特征向量 $x_k$ 使得新的残差向量 $residual$ 与这三个特征向量的相关性相等（即残差向量等于这三个特征向量的等角向量上），以此类推。\n",
    "\n",
    "**第五步：**当残差向量 $residual$ 足够小或者所有特征向量都已被选择，结束迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![最小角回归.jpeg](./img/最小角回归.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图演示了只有两个特征向量时的情况，初始残差向量为 $y_2$，其中 $x_1$ 向量与残差向量的相关性最大（向量夹角最小），找到一个 $θ_{11}$ 使得新的残差向量 $y_2 - x_1 * θ_{11}$ 是 $x_1$ 和 $x_2$ 的角平分线（图中为 $u_2$），此时 $w_1 = θ_{11}$, $w_2 = 0$。最后找到一组 $θ_{21}$， $θ_{22}$ 使得新的残差向量 $y_2 - x_1 * θ_{11} - (x_1 * θ_{21} + x_2 * θ_{22})$ 尽可能小， 此时 $w_1 = θ_{11} + θ_{21}$， $w_2 = θ_{22}$。所有特征向量都已被选择，所以结束迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "坐标下降法相对最小角回归法相对好理解一些，每次只需要关心一个权重系数即可。最小角回归法则是通过一些巧妙的数学公式减少了迭代次数，使其的最坏计算复杂度和最小二乘法类似。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge（L2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法基本思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ridge regression = linear regression + L2 regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**假设函数**（同LR）\n",
    "$$h_\\theta(x)=\\theta_0+\\theta_1x = \\theta x$$\n",
    "\n",
    "**损失函数**\n",
    "$$J_0(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "$$J(\\theta)=J_0(\\theta)+\\lambda\\sum_{j=1}^n\\theta_j^2$$\n",
    "\n",
    "$$=J_0(\\theta)+\\lambda\\lVert\\theta_j\\rVert_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**最优解**\n",
    "\n",
    "由于正则项可导，所以求解步骤与LR类似。\n",
    "\n",
    "$$\\theta = (X^TX+\\lambda I)^{-1}X^Ty\\ \\ \\lambda\\in\\Re$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElasticNet（弹性网络）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "弹性网络是L1、L2正则化的组合。\n",
    "\n",
    "保留了Lasso回归的特征选择的性质，又兼顾了岭回归的稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**假设函数**（同LR）\n",
    "$$h_\\theta(x)=\\theta_0+\\theta_1x = \\theta x$$\n",
    "\n",
    "**损失函数**\n",
    "$$J_0(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "$$J(\\theta)=J_0(\\theta)+\\alpha\\rho\\lVert\\theta\\rVert_1+\\frac{\\alpha(1-\\rho)}{2}\\lVert\\theta\\rVert_2^2$$\n",
    "\n",
    "通过调整 $\\rho$ 的大小来使之更偏向Lasso or 岭回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn中`ElasticNetCV` 类可以通过交叉验证来设置参数 $\\alpha$ 和 $\\rho$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**拟合算法**\n",
    "\n",
    "坐标下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lasso回归与岭回归（ridge） 两者的区别**\n",
    "\n",
    "随着正则化强度的增大，$\\theta$ 的取值会逐渐变小，L1正则化会将参数压缩到 0 ，L2正则化只会让参数尽量小，不会取到 0 。所以在L1正则化在逐渐加强的过程中，相对不重要的特征的参数会比相对重要的特征的参数更快地变成 0 。所以L1正则化本质是一个特征选择的过程。选出少量但重要的特征，以防止过拟合问题。而L2正则化在加强的过程中，会尽量让每个特征对模型都有一些贡献，相对不重要的特征的参数会非常接近 0 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso回归算法 https://blog.csdn.net/sai_simon/article/details/122359015\n",
    "\n",
    "岭回归算法 https://blog.csdn.net/sai_simon/article/details/122337097?spm=1001.2014.3001.5501\n",
    "\n",
    "弹性网络回归算法 https://blog.csdn.net/sai_simon/article/details/122376407"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

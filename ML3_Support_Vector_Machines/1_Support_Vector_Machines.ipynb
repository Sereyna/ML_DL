{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概念定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在基于训练集D的样本空间中找到一个划分超平面，将不同的类别区分开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**超平面定义**\n",
    "\n",
    "$$\\omega^Tx+b=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**样本空间中 $x$ 到超平面 $(\\omega,b)$ 的距离为**\n",
    "\n",
    "\n",
    "$$r=\\frac{|\\omega^Tx+b|}{\\lVert\\omega\\rVert}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**超平面 $(\\omega,b)$ 分类的表达式**\n",
    "\n",
    "$$\\begin{cases} \\omega^Tx+b\\ge +1,\\ y_i = +1\\\\ \\omega^Tx+b\\le-1,\\ y_i = -1 \\end{cases}$$\n",
    "\n",
    "$$\\Downarrow$$\n",
    "\n",
    "$$y_i(\\omega^Tx_i+b)\\ge 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设两个超平面 $A:\\omega^Tx+b=+1$ $C:\\omega^Tx+b=-1$ 使得两个超平面之间距离最大，同时样本分类正确（$\\because$ $\\omega,b$ 可等比例缩放，$\\therefore$ 一定有满足上式的 $\\omega,b$ 值）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**超平面A,C之间的距离**\n",
    "\n",
    "$$\\gamma = \\frac{2}{\\lVert\\omega\\rVert}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**问题定义**\n",
    "\n",
    "（式1.1-1）\n",
    "\n",
    "$$\\underset{\\omega,b}{max}\\frac{2}{\\lVert\\omega\\rVert}$$\n",
    "\n",
    "$$s.t.\\ \\ y_i(\\omega^Tx_i+b)\\ge 1\\ \\ \\ i=1,2,\\cdots,N$$\n",
    "\n",
    "$$\\Downarrow{向量内积的形式}$$\n",
    "\n",
    "$$\\underset{\\omega,b}{min}\\frac{1}{2}\\omega^T\\omega$$\n",
    "\n",
    "$$s.t.\\ \\ y_i(\\omega^Tx_i+b)\\ge 1\\ \\ \\ i=1,2,\\cdots,N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上式为是SVM的基本型\n",
    "\n",
    "（式1.1-1）是一个凸二次规划问题，能直接用现成的优化计算包求解，但有更高效的办法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对偶问题求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一步：**对（式1.1-1）使用拉格朗日乘子法可得到其对偶问题 $=$ 对（式1.1-1）每条约束添加拉格朗日乘子 $\\alpha_i\\ge 0$ ,则该问题的拉格朗日函数可写为：\n",
    "\n",
    "（式1.2-1）\n",
    "\n",
    "$$L(\\omega,b,\\alpha)=\\frac{1}{2}\\lVert\\omega\\rVert^2+\\sum_{i=1}^m\\alpha_i(1-y_i(\\omega^Tx_i+b))\\ \\ \\ \\alpha=(\\alpha_1;\\alpha_2;\\cdots;\\alpha_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二步：** $L(\\omega,b,\\alpha)$ 对 $\\omega,b$ 的偏导为零可得\n",
    "\n",
    "（式1.2-2）\n",
    "\n",
    "$$\\omega=\\sum_{i=1}^m\\alpha_iy_ix_i$$\n",
    "\n",
    "（式1.2-3）\n",
    "\n",
    "$$0=\\sum_{i=1}^m\\alpha_iy_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第三步：**将（式1.2-2）代入（式1.2-1）可将 $\\omega,b$ 消去，再加上（式1.2-3）的约束可得\n",
    "\n",
    "$$L(\\alpha)=\\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_iy_ix_i^T\\alpha_jy_jx_j$$\n",
    "\n",
    "将上式代入到最初的对偶问题\n",
    "\n",
    "（式1.2-4）\n",
    "\n",
    "$$\\underset{\\alpha}{max}\\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx_i^Tx_j$$\n",
    "\n",
    "$$s.t.\\ \\ \\sum_{i=1}^m\\alpha_iy_i=0$$\n",
    "\n",
    "$$\\alpha_i\\ge0,\\ i=1,2,\\cdots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要想（式1.2-4）与（式1.1-1）问题等价，需满足 $KKT$ 条件\n",
    "\n",
    "$$\\begin{cases}\\nabla_{\\omega,b}L(\\omega,b,\\alpha)=0\\\\\\alpha_i\\ge0\\\\y_if(x_i)-1\\ge0\\\\\\alpha_i(y_if(x_i)-1)=0\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第四步：**根据上式解出 $\\alpha$ ,求出 $\\omega,b$ 即可得模型\n",
    "\n",
    "$$f(x)=\\omega^Tx+b$$\n",
    "\n",
    "$$=\\sum_{i=1}^m\\alpha_iy_ix_i^Tx+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMO — 序列最小优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential minimal optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_1.2中第四步如何求解？_**如何根据（式1.2-4）求出 $\\alpha$ ？\n",
    "\n",
    "虽然可以使用通用的二次规划算法来求解，但该问题的规模正比于训练样本数，在实际中会造成很大开销，为避开这个障碍，提出SMO算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基本思路**：不断执行如下两个步骤直至收敛\n",
    "1. 选取一对需更新的变量 $\\alpha_i$ 和 $\\alpha_j$；\n",
    "2. 固定 $\\alpha_i$ 和 $\\alpha_j$ 以外的参数，求解（式1.2-4）获得更新后的 $\\alpha_i$ 和 $\\alpha_j$；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 软间隔与正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述模型为“硬间隔”模型，即假设样本线性可分，所有样本都可以正确归类，但在实际中：\n",
    "- 很难确定合适的核函数使得训练样本在特征空间中完全线性可分\n",
    "- 即使能找到合适的核函数，也很难确定这个貌似线性可分的结果不是由于过拟合造成的。\n",
    "\n",
    "缓解上述问题的一个办法是允许SVM在一些样本上出错，即要引入“软间隔”的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改（式1.1-1），加上分类错误样本点惩罚项，得\n",
    "\n",
    "（式2.1-1）\n",
    "\n",
    "$$\\underset{\\omega,b}{min}\\frac{1}{2}\\omega^T\\omega+C\\sum_{i=1}^m\\ell_{0/1}(y_i(\\omega^Tx_i+b)-1)$$\n",
    "\n",
    "$$s.t.\\ \\ \\ C>0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $C>0$ 是常数， $\\ell_{0/1}$ 是“0/1损失函数”\n",
    "\n",
    "$$\\ell_{0/1}(z)=\\begin{cases}1,\\ if\\ \\ z<0;\\\\0,\\ otherwise\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当 $C \\rightarrow\\infty$ ，（式2.1-1）等价于（式1.1-1）；\n",
    "\n",
    "当 $C$ 取有限值时，（式2.1-1）允许一些样本不满足约束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**替代损失**\n",
    "\n",
    "由于 $\\ell_{0/1}$ 非凸、非连续，数学性质不好，使得（式2.1-1）不易求解，所以常使用其他函数替代 $\\ell_{0/1}$，称为“替代损失”，其通常具有良好的数学性质：\n",
    "- $ hinge\\ loss : \\ell_{hinge}(z)=max(0,1-z)$\n",
    "- $exp\\ los : \\ell_{exp}(z)=exp(-z)$\n",
    "- $logistic\\ loss : \\ell_{log}(z)=log(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以 $ hinge\\ loss $为例，（式2.1-1）可修改为：\n",
    "\n",
    "（式2.1-2）\n",
    "\n",
    "$$\\underset{\\omega,b}{min}\\frac{1}{2}\\omega^T\\omega+C\\sum_{i=1}^mmax(0,1-y_i(\\omega^Tx_i+b))$$\n",
    "\n",
    "$$s.t.\\ \\ \\ C>0$$\n",
    "\n",
    "$$\\Downarrow{引入松弛变量 \\xi_i\\ge0}$$\n",
    "\n",
    "$$\\underset{\\omega,b,\\xi_i}{min}\\frac{1}{2}\\omega^T\\omega+C\\sum_{i=1}^m\\xi_i$$\n",
    "\n",
    "$$s.t.\\ \\ \\ y_i(\\omega^Tx_i+b)\\ge1-\\xi_i$$\n",
    "\n",
    "$$\\xi_i\\ge0,i=1,2,\\cdots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上即是软间隔SVM的问题定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对偶问题求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这仍是一个二次规划问题，通过对偶问题求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一步：**使用拉格朗日乘子法得到拉格朗日函数\n",
    "\n",
    "$$L(\\omega,b,\\alpha,\\xi,\\mu)=\\frac{1}{2}\\lVert\\omega\\rVert^2+C\\sum_{i=1}^m\\xi_i+\\sum_{i=1}^m\\alpha_i(1-\\xi_i-y_i(\\omega^Tx_i+b))-\\sum_{i=1}^m\\mu_i\\xi_i$$\n",
    "\n",
    "其中 $\\alpha_i\\ge0, \\mu_i\\ge0$ 是拉格朗日乘子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二步：**$L(\\omega,b,\\alpha,\\xi,\\mu)$ 对 $\\omega,b，\\xi_i$ 的偏导为零可得\n",
    "\n",
    "$$\\omega=\\sum_{i=1}^m\\alpha_iy_ix_i$$\n",
    "\n",
    "$$0=\\sum_{i=1}^m\\alpha_iy_i$$\n",
    "\n",
    "$$C=\\alpha_i+\\mu_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第三步：**将第二步所得代入原始问题定义\n",
    "\n",
    "（式2.2-1）\n",
    "\n",
    "$$\\underset{\\alpha}{max}\\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jx_i^Tx_j$$\n",
    "\n",
    "$$s.t.\\ \\ \\sum_{i=1}^m\\alpha_iy_i=0$$\n",
    "\n",
    "$$0\\le\\alpha_i\\le C,\\ \\ i=1,2,\\cdots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "唯一与硬间隔不同的地方在于 $\\alpha_i$ 的约束不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的，要想（式2.1-2）与（式2.2-1）问题等价，需满足 $KKT$ 条件\n",
    "\n",
    "$$\\begin{cases}\\alpha_i\\ge0,\\ \\mu_i\\ge0\\\\y_if(x_i)-1+\\xi_i\\ge0\\\\\\alpha_i(y_if(x_i)-1+\\xi_i)=0\\\\\\xi_i\\ge0,\\ \\mu_i\\xi_i=0\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法思想与步骤与硬间隔相同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上讨论的是在训练样本组成的特征空间中，样本线性可分的情况，如样本在当前维度的空间中线性不可分，则需要将其映射到可以使其线性可分的高维空间。\n",
    "\n",
    ">如果原始空间是有限维，即属性数量有限，那么一定存在一个高维特征空间使样本可分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设存在一个函数 $\\phi(x)$ 表示 $x$ 映射后的特征向量，超平面模型可表示为\n",
    "\n",
    "$$f(x)=\\omega^T\\phi(x)+b$$\n",
    "\n",
    "类似（式1.1-1）的问题定义：\n",
    "\n",
    "（式3.1-1）\n",
    "\n",
    "$$\\underset{\\omega,b}{min}\\frac{1}{2}\\omega^T\\omega$$\n",
    "\n",
    "$$s.t.\\ \\ y_i(\\omega^T\\phi(x_i)+b)\\ge 1\\ \\ \\ i=1,2,\\cdots,N$$\n",
    "\n",
    "类似（式2.1-2）的问题定义：\n",
    "\n",
    "（式3.1-2）\n",
    "\n",
    "$$\\underset{\\omega,b,\\xi_i}{min}\\frac{1}{2}\\omega^T\\omega+C\\sum_{i=1}^m\\xi_i$$\n",
    "\n",
    "$$s.t.\\ \\ \\ y_i(\\omega^T\\phi(x_i)+b)\\ge1-\\xi_i$$\n",
    "\n",
    "$$\\xi_i\\ge0,i=1,2,\\cdots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对偶模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（式3.1-2）的对偶模型为\n",
    "\n",
    "（式3.2-1）\n",
    "\n",
    "$$\\underset{\\alpha}{max}\\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_j\\phi(x_i)^T\\phi(x_j)$$\n",
    "\n",
    "$$s.t.\\ \\ \\sum_{i=1}^m\\alpha_iy_i=0$$\n",
    "\n",
    "$$0\\le\\alpha_i\\le C,\\ \\ i=1,2,\\cdots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个函数：\n",
    "\n",
    "$$K(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$$\n",
    "\n",
    "这个转换称为“核技巧”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（式3.2-1）可重写为\n",
    "\n",
    "$$\\underset{\\alpha}{max}\\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i\\alpha_jy_iy_jK(x_i,x_j)$$\n",
    "\n",
    "$$s.t.\\ \\ \\sum_{i=1}^m\\alpha_iy_i=0$$\n",
    "\n",
    "$$0\\le\\alpha_i\\le C,\\ \\ i=1,2,\\cdots,m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求解后即可得到\n",
    "\n",
    "$$f(x)=\\omega^T\\phi(x)+b$$\n",
    "\n",
    "$$=\\sum_{i=1}^m\\alpha_iy_i\\phi(x_i)^T\\phi(x)+b$$\n",
    "\n",
    "$$=\\sum_{i=1}^m\\alpha_iy_iK(x_i,x_j)+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**核函数**\n",
    "![kernel](img/kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变化不大，同之前解法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机的优势在于:\n",
    "\n",
    "- 在高维空间中非常高效.\n",
    "- 即使在数据维度比样本数量大的情况下仍然有效.\n",
    "- 在决策函数（称为支持向量）中使用训练集的子集,因此它也是高效利用内存的.\n",
    "- 通用性: 不同的核函数 核函数 与特定的决策函数一一对应.常见的 kernel 已经提供,也可以指定定制的内核.\n",
    "\n",
    "支持向量机的缺点包括:\n",
    "\n",
    "- 如果特征数量比样本数量大得多,在选择核函数 核函数 时要避免过拟合, 而且正则化项是非常重要的.\n",
    "- 支持向量机不直接提供概率估计,这些都是使用昂贵的五次交叉验算计算的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 公式推导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2第三步**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(\\omega,b,\\alpha)=\\frac{1}{2}\\omega^T\\omega+\\sum_{i=1}^m\\alpha_i(1-y_i(\\omega^Tx_i+b))$$\n",
    "\n",
    "\n",
    "$$L(\\alpha)=\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_iy_ix_i^T\\alpha_jy_jx_j+\\sum_{i=1}^m\\alpha_i-\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_iy_i\\alpha_jy_jx_i^Tx_j-\\sum_{i=1}^m\\alpha_iy_ib$$\n",
    "\n",
    "$$=\\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_iy_ix_i^T\\alpha_jy_jx_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Downarrow{代入最初对偶问题}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\underset{\\omega,b}{min}\\frac{1}{2}\\omega^T\\omega\\ \\ \\ s.t.\\ \\ y_i(\\omega^Tx_i+b)\\ge 1\\ \\ \\ i=1,2,\\cdots,N$$\n",
    "\n",
    "$$\\Rightarrow\\underset{\\omega,b}{min}\\underset{\\alpha}{max}L(\\omega,b,\\alpha)\\ \\ \\ s.t.\\ \\ \\alpha\\ge0$$\n",
    "\n",
    "$$\\Rightarrow\\underset{\\alpha}{max}\\underset{\\omega,b}{min}L(\\omega,b,\\alpha)\\ \\ \\ s.t.\\ \\ \\alpha\\ge0$$\n",
    "\n",
    "$$\\Rightarrow\\underset{\\alpha}{max}L(\\alpha)\\ \\ \\ s.t.\\ \\ \\alpha\\ge0$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
